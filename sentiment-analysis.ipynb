{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:15:50.264030Z",
     "iopub.status.busy": "2022-11-29T11:15:50.263719Z",
     "iopub.status.idle": "2022-11-29T11:15:50.268228Z",
     "shell.execute_reply": "2022-11-29T11:15:50.267176Z",
     "shell.execute_reply.started": "2022-11-29T11:15:50.264007Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:15:50.728043Z",
     "iopub.status.busy": "2022-11-29T11:15:50.727736Z",
     "iopub.status.idle": "2022-11-29T11:15:51.268119Z",
     "shell.execute_reply": "2022-11-29T11:15:51.266672Z",
     "shell.execute_reply.started": "2022-11-29T11:15:50.728019Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/Mouni/Pattern_project/input/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:15:51.271718Z",
     "iopub.status.busy": "2022-11-29T11:15:51.271269Z",
     "iopub.status.idle": "2022-11-29T11:15:51.285589Z",
     "shell.execute_reply": "2022-11-29T11:15:51.283729Z",
     "shell.execute_reply.started": "2022-11-29T11:15:51.271686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T12:04:30.264277Z",
     "iopub.status.busy": "2022-11-29T12:04:30.263970Z",
     "iopub.status.idle": "2022-11-29T12:04:30.283548Z",
     "shell.execute_reply": "2022-11-29T12:04:30.282355Z",
     "shell.execute_reply.started": "2022-11-29T12:04:30.264253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Duplicate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T12:04:32.585157Z",
     "iopub.status.busy": "2022-11-29T12:04:32.584825Z",
     "iopub.status.idle": "2022-11-29T12:04:32.758749Z",
     "shell.execute_reply": "2022-11-29T12:04:32.757771Z",
     "shell.execute_reply.started": "2022-11-29T12:04:32.585132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate Samples :  418\n"
     ]
    }
   ],
   "source": [
    "duplicate_rows = df[df.duplicated()]\n",
    "print('Number of duplicate Samples : ',len(duplicate_rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T12:05:56.114774Z",
     "iopub.status.busy": "2022-11-29T12:05:56.114458Z",
     "iopub.status.idle": "2022-11-29T12:05:56.285330Z",
     "shell.execute_reply": "2022-11-29T12:05:56.283707Z",
     "shell.execute_reply.started": "2022-11-29T12:05:56.114750Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.reset_index(inplace=True)\n",
    "df = df.drop(['index'],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T12:06:04.162363Z",
     "iopub.status.busy": "2022-11-29T12:06:04.161809Z",
     "iopub.status.idle": "2022-11-29T12:06:04.326491Z",
     "shell.execute_reply": "2022-11-29T12:06:04.325570Z",
     "shell.execute_reply.started": "2022-11-29T12:06:04.162324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate Samples :  0\n"
     ]
    }
   ],
   "source": [
    "duplicate_rows = df[df.duplicated()]\n",
    "print('Number of duplicate Samples : ',len(duplicate_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:15:51.287764Z",
     "iopub.status.busy": "2022-11-29T11:15:51.287359Z",
     "iopub.status.idle": "2022-11-29T11:15:51.303614Z",
     "shell.execute_reply": "2022-11-29T11:15:51.302091Z",
     "shell.execute_reply.started": "2022-11-29T11:15:51.287736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    24884\n",
       "negative    24698\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:15:51.516263Z",
     "iopub.status.busy": "2022-11-29T11:15:51.515910Z",
     "iopub.status.idle": "2022-11-29T11:15:51.525485Z",
     "shell.execute_reply": "2022-11-29T11:15:51.524145Z",
     "shell.execute_reply.started": "2022-11-29T11:15:51.516238Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"review\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:15:51.795276Z",
     "iopub.status.busy": "2022-11-29T11:15:51.794939Z",
     "iopub.status.idle": "2022-11-29T11:15:51.800358Z",
     "shell.execute_reply": "2022-11-29T11:15:51.799042Z",
     "shell.execute_reply.started": "2022-11-29T11:15:51.795251Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:15:51.960835Z",
     "iopub.status.busy": "2022-11-29T11:15:51.960516Z",
     "iopub.status.idle": "2022-11-29T11:15:51.968576Z",
     "shell.execute_reply": "2022-11-29T11:15:51.967669Z",
     "shell.execute_reply.started": "2022-11-29T11:15:51.960810Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = BeautifulSoup(df[\"review\"][0], 'html').get_text()\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Decontractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:15:52.317977Z",
     "iopub.status.busy": "2022-11-29T11:15:52.317627Z",
     "iopub.status.idle": "2022-11-29T11:16:00.922219Z",
     "shell.execute_reply": "2022-11-29T11:16:00.920754Z",
     "shell.execute_reply.started": "2022-11-29T11:15:52.317952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\mouni\\anaconda3\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\mouni\\anaconda3\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\mouni\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\mouni\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:16:00.925594Z",
     "iopub.status.busy": "2022-11-29T11:16:00.925218Z",
     "iopub.status.idle": "2022-11-29T11:16:00.934705Z",
     "shell.execute_reply": "2022-11-29T11:16:00.933464Z",
     "shell.execute_reply.started": "2022-11-29T11:16:00.925566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One of the other reviewers has mentioned that after watching just 1 Oz episode you will be hooked. They are right, as this is exactly what happened with me.The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Them City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.I would say the main appeal of the show is due to the fact that it goes where other shows would not dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ does not mess around. The first episode I ever saw struck me as so nasty it was surreal, I could not say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who will be sold out for a nickel, inmates who will kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....that is if you can get in touch with your darker side.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = \" \".join([contractions.fix(x) for x in output.split(\" \")])\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing All Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:16:00.936672Z",
     "iopub.status.busy": "2022-11-29T11:16:00.936400Z",
     "iopub.status.idle": "2022-11-29T11:16:00.951686Z",
     "shell.execute_reply": "2022-11-29T11:16:00.950179Z",
     "shell.execute_reply.started": "2022-11-29T11:16:00.936647Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One of the other reviewers has mentioned that after watching just Oz episode you will be hooked They are right as this is exactly what happened with me The first thing that struck me about Oz was its brutality and unflinching scenes of violence which set in right from the word GO Trust me this is not a show for the faint hearted or timid This show pulls no punches with regards to drugs sex or violence Its is hardcore in the classic use of the word It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary It focuses mainly on Emerald City an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda Them City is home to many Aryans Muslims gangstas Latinos Christians Italians Irish and more so scuffles death stares dodgy dealings and shady agreements are never far away I would say the main appeal of the show is due to the fact that it goes where other shows would not dare Forget pretty pictures painted for mainstream audiences forget charm forget romance OZ does not mess around The first episode I ever saw struck me as so nasty it was surreal I could not say I was ready for it but as I watched more I developed a taste for Oz and got accustomed to the high levels of graphic violence Not just violence but injustice crooked guards who will be sold out for a nickel inmates who will kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience Watching Oz you may become comfortable with what is uncomfortable viewing that is if you can get in touch with your darker side '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = re.sub('[^A-Za-z]+', ' ', output)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop words and Applying Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:16:00.955888Z",
     "iopub.status.busy": "2022-11-29T11:16:00.954818Z",
     "iopub.status.idle": "2022-11-29T11:16:01.018045Z",
     "shell.execute_reply": "2022-11-29T11:16:01.016417Z",
     "shell.execute_reply.started": "2022-11-29T11:16:00.955860Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mouni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Mouni\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:16:01.019790Z",
     "iopub.status.busy": "2022-11-29T11:16:01.019498Z",
     "iopub.status.idle": "2022-11-29T11:16:01.026108Z",
     "shell.execute_reply": "2022-11-29T11:16:01.024851Z",
     "shell.execute_reply.started": "2022-11-29T11:16:01.019764Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mouni\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:16:01.028432Z",
     "iopub.status.busy": "2022-11-29T11:16:01.027982Z",
     "iopub.status.idle": "2022-11-29T11:16:01.077076Z",
     "shell.execute_reply": "2022-11-29T11:16:01.075025Z",
     "shell.execute_reply.started": "2022-11-29T11:16:01.028406Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one reviewer mentioned watching oz episode hooked right exactly happened first thing struck oz brutality unflinching scene violence set right word go trust show faint hearted timid show pull punch regard drug sex violence hardcore classic use word called oz nickname given oswald maximum security state penitentary focus mainly emerald city experimental section prison cell glass front face inwards privacy high agenda city home many aryan muslim gangsta latino christian italian irish scuffle death stare dodgy dealing shady agreement never far away would say main appeal show due fact go show would dare forget pretty picture painted mainstream audience forget charm forget romance oz mess around first episode ever saw struck nasty surreal could say ready watched developed taste oz got accustomed high level graphic violence violence injustice crooked guard sold nickel inmate kill order get away well mannered middle class inmate turned prison bitch due lack street skill prison experience watching oz may become comfortable uncomfortable viewing get touch darker side'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = ' '.join(lemmatizer.lemmatize(e.lower()) for e in output.split() if e.lower() not in stopwords.words('english'))\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between Original Text and Preprocessed Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:16:01.079434Z",
     "iopub.status.busy": "2022-11-29T11:16:01.079125Z",
     "iopub.status.idle": "2022-11-29T11:16:01.085740Z",
     "shell.execute_reply": "2022-11-29T11:16:01.084583Z",
     "shell.execute_reply.started": "2022-11-29T11:16:01.079408Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"review\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:16:01.087966Z",
     "iopub.status.busy": "2022-11-29T11:16:01.087648Z",
     "iopub.status.idle": "2022-11-29T11:16:01.100236Z",
     "shell.execute_reply": "2022-11-29T11:16:01.098963Z",
     "shell.execute_reply.started": "2022-11-29T11:16:01.087934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one reviewer mentioned watching oz episode hooked right exactly happened first thing struck oz brutality unflinching scene violence set right word go trust show faint hearted timid show pull punch regard drug sex violence hardcore classic use word called oz nickname given oswald maximum security state penitentary focus mainly emerald city experimental section prison cell glass front face inwards privacy high agenda city home many aryan muslim gangsta latino christian italian irish scuffle death stare dodgy dealing shady agreement never far away would say main appeal show due fact go show would dare forget pretty picture painted mainstream audience forget charm forget romance oz mess around first episode ever saw struck nasty surreal could say ready watched developed taste oz got accustomed high level graphic violence violence injustice crooked guard sold nickel inmate kill order get away well mannered middle class inmate turned prison bitch due lack street skill prison experience watching oz may become comfortable uncomfortable viewing get touch darker side'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:16:01.103791Z",
     "iopub.status.busy": "2022-11-29T11:16:01.102749Z",
     "iopub.status.idle": "2022-11-29T11:16:01.115177Z",
     "shell.execute_reply": "2022-11-29T11:16:01.113775Z",
     "shell.execute_reply.started": "2022-11-29T11:16:01.103721Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    output = BeautifulSoup(text, 'html').get_text()\n",
    "    output = \" \".join([contractions.fix(x) for x in output.split(\" \")])\n",
    "    output = re.sub('[^A-Za-z]+', ' ', output)\n",
    "    output = ' '.join(lemmatizer.lemmatize(e.lower()) for e in output.split() if e.lower() not in stopwords.words('english'))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:16:01.120061Z",
     "iopub.status.busy": "2022-11-29T11:16:01.119745Z",
     "iopub.status.idle": "2022-11-29T11:16:01.126794Z",
     "shell.execute_reply": "2022-11-29T11:16:01.125605Z",
     "shell.execute_reply.started": "2022-11-29T11:16:01.120038Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = df.iloc[0:500,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:16:01.128905Z",
     "iopub.status.busy": "2022-11-29T11:16:01.128533Z",
     "iopub.status.idle": "2022-11-29T11:38:59.980359Z",
     "shell.execute_reply": "2022-11-29T11:38:59.979515Z",
     "shell.execute_reply.started": "2022-11-29T11:16:01.128879Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 870/49582 [02:11<1:29:30,  9.07it/s]C:\\Users\\Mouni\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "100%|██████████| 49582/49582 [2:35:55<00:00,  5.30it/s]    \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "text = []\n",
    "for s in tqdm(df['review'].values):\n",
    "    text.append(preprocess(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:38:59.981719Z",
     "iopub.status.busy": "2022-11-29T11:38:59.981426Z",
     "iopub.status.idle": "2022-11-29T11:39:00.016184Z",
     "shell.execute_reply": "2022-11-29T11:39:00.014736Z",
     "shell.execute_reply.started": "2022-11-29T11:38:59.981694Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"label\"] = df[\"sentiment\"].apply(lambda x: [\"negative\",\"positive\"].index(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:39:00.017709Z",
     "iopub.status.busy": "2022-11-29T11:39:00.017471Z",
     "iopub.status.idle": "2022-11-29T11:39:04.924213Z",
     "shell.execute_reply": "2022-11-29T11:39:04.923150Z",
     "shell.execute_reply.started": "2022-11-29T11:39:00.017685Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=50000)\n",
    "data_bow = vectorizer.fit_transform(text)\n",
    "\n",
    "bow_x_train, bow_x_test,bow_y_train,bow_y_test = train_test_split(data_bow , df[\"label\"].values, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:39:04.925772Z",
     "iopub.status.busy": "2022-11-29T11:39:04.925497Z",
     "iopub.status.idle": "2022-11-29T11:39:05.440226Z",
     "shell.execute_reply": "2022-11-29T11:39:05.438472Z",
     "shell.execute_reply.started": "2022-11-29T11:39:04.925747Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "model = TfidfTransformer()\n",
    "\n",
    "tfidf = model.fit_transform(data_bow)\n",
    "\n",
    "tfidf_x_train, tfidf_x_test,tfidf_y_train,tfidf_y_test = train_test_split(tfidf, df[\"label\"].values, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:39:05.442138Z",
     "iopub.status.busy": "2022-11-29T11:39:05.441750Z",
     "iopub.status.idle": "2022-11-29T11:39:28.819220Z",
     "shell.execute_reply": "2022-11-29T11:39:28.817839Z",
     "shell.execute_reply.started": "2022-11-29T11:39:05.442104Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "split_text = [x.split(\" \") for x in text]\n",
    "\n",
    "model = Word2Vec(split_text,min_count=5,vector_size=64, workers=4)\n",
    "\n",
    "embedding_words = list(model.wv.key_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:39:28.820871Z",
     "iopub.status.busy": "2022-11-29T11:39:28.820574Z",
     "iopub.status.idle": "2022-11-29T11:39:28.827244Z",
     "shell.execute_reply": "2022-11-29T11:39:28.826063Z",
     "shell.execute_reply.started": "2022-11-29T11:39:28.820848Z"
    }
   },
   "outputs": [],
   "source": [
    "def average_vector(splitted_text):\n",
    "    \n",
    "    sentance_vector = np.zeros(64) \n",
    "    no_words =0; \n",
    "    for word in splitted_text: \n",
    "        if word in embedding_words:\n",
    "            vec = model.wv[word]\n",
    "            sentance_vector += vec\n",
    "            no_words += 1\n",
    "    if no_words != 0:\n",
    "        sentance_vector /= no_words\n",
    "    return sentance_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:39:28.829203Z",
     "iopub.status.busy": "2022-11-29T11:39:28.828888Z",
     "iopub.status.idle": "2022-11-29T11:49:20.584619Z",
     "shell.execute_reply": "2022-11-29T11:49:20.583116Z",
     "shell.execute_reply.started": "2022-11-29T11:39:28.829177Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49582/49582 [11:00<00:00, 75.07it/s] \n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "for x in tqdm(split_text):\n",
    "    vectors.append(average_vector(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:49:20.586743Z",
     "iopub.status.busy": "2022-11-29T11:49:20.586075Z",
     "iopub.status.idle": "2022-11-29T11:49:20.609949Z",
     "shell.execute_reply": "2022-11-29T11:49:20.608489Z",
     "shell.execute_reply.started": "2022-11-29T11:49:20.586687Z"
    }
   },
   "outputs": [],
   "source": [
    "word2vec_x_train, word2vec_x_test,word2vec_y_train,word2vec_y_test = train_test_split(vectors, df[\"label\"].values, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:49:20.612089Z",
     "iopub.status.busy": "2022-11-29T11:49:20.611703Z",
     "iopub.status.idle": "2022-11-29T11:49:29.645924Z",
     "shell.execute_reply": "2022-11-29T11:49:29.644828Z",
     "shell.execute_reply.started": "2022-11-29T11:49:20.612053Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mouni\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Model\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     19759\n",
      "           1       0.99      0.99      0.99     19906\n",
      "\n",
      "    accuracy                           0.99     39665\n",
      "   macro avg       0.99      0.99      0.99     39665\n",
      "weighted avg       0.99      0.99      0.99     39665\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88      4939\n",
      "           1       0.88      0.89      0.88      4978\n",
      "\n",
      "    accuracy                           0.88      9917\n",
      "   macro avg       0.88      0.88      0.88      9917\n",
      "weighted avg       0.88      0.88      0.88      9917\n",
      "\n",
      "Term Frequency Inverse Document Frequency Model\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93     19759\n",
      "           1       0.92      0.94      0.93     19906\n",
      "\n",
      "    accuracy                           0.93     39665\n",
      "   macro avg       0.93      0.93      0.93     39665\n",
      "weighted avg       0.93      0.93      0.93     39665\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.89      4939\n",
      "           1       0.88      0.91      0.90      4978\n",
      "\n",
      "    accuracy                           0.89      9917\n",
      "   macro avg       0.89      0.89      0.89      9917\n",
      "weighted avg       0.89      0.89      0.89      9917\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mouni\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec Model\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.86     19759\n",
      "           1       0.85      0.87      0.86     19906\n",
      "\n",
      "    accuracy                           0.86     39665\n",
      "   macro avg       0.86      0.86      0.86     39665\n",
      "weighted avg       0.86      0.86      0.86     39665\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.84      0.85      4939\n",
      "           1       0.85      0.86      0.85      4978\n",
      "\n",
      "    accuracy                           0.85      9917\n",
      "   macro avg       0.85      0.85      0.85      9917\n",
      "weighted avg       0.85      0.85      0.85      9917\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lg_bow = LogisticRegression(random_state=0).fit(bow_x_train, bow_y_train)\n",
    "\n",
    "print(\"Bag of Words Model\")\n",
    "print(\" \")\n",
    "print(classification_report(bow_y_train,lg_bow.predict(bow_x_train)))\n",
    "print(classification_report(bow_y_test,lg_bow.predict(bow_x_test)))\n",
    "\n",
    "\n",
    "lg_tf_idf = LogisticRegression(random_state=0).fit(tfidf_x_train, tfidf_y_train)\n",
    "\n",
    "print(\"Term Frequency Inverse Document Frequency Model\")\n",
    "print(\" \")\n",
    "print(classification_report(tfidf_y_train,lg_tf_idf.predict(tfidf_x_train)))\n",
    "print(classification_report(tfidf_y_test,lg_tf_idf.predict(tfidf_x_test)))\n",
    "\n",
    "\n",
    "lg_word2vec = LogisticRegression(random_state=0).fit(word2vec_x_train, word2vec_y_train)\n",
    "\n",
    "print(\"Word2vec Model\")\n",
    "print(\" \")\n",
    "print(classification_report(word2vec_y_train,lg_word2vec.predict(word2vec_x_train)))\n",
    "print(classification_report(word2vec_y_test,lg_word2vec.predict(word2vec_x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:49:29.647777Z",
     "iopub.status.busy": "2022-11-29T11:49:29.647095Z",
     "iopub.status.idle": "2022-11-29T11:51:57.699930Z",
     "shell.execute_reply": "2022-11-29T11:51:57.698136Z",
     "shell.execute_reply.started": "2022-11-29T11:49:29.647743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Model\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19759\n",
      "           1       1.00      1.00      1.00     19906\n",
      "\n",
      "    accuracy                           1.00     39665\n",
      "   macro avg       1.00      1.00      1.00     39665\n",
      "weighted avg       1.00      1.00      1.00     39665\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.72      0.73      4939\n",
      "           1       0.73      0.73      0.73      4978\n",
      "\n",
      "    accuracy                           0.73      9917\n",
      "   macro avg       0.73      0.73      0.73      9917\n",
      "weighted avg       0.73      0.73      0.73      9917\n",
      "\n",
      "Term Frequency Inverse Document Frequency Model\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19759\n",
      "           1       1.00      1.00      1.00     19906\n",
      "\n",
      "    accuracy                           1.00     39665\n",
      "   macro avg       1.00      1.00      1.00     39665\n",
      "weighted avg       1.00      1.00      1.00     39665\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71      4939\n",
      "           1       0.72      0.71      0.72      4978\n",
      "\n",
      "    accuracy                           0.71      9917\n",
      "   macro avg       0.71      0.71      0.71      9917\n",
      "weighted avg       0.71      0.71      0.71      9917\n",
      "\n",
      "Word2vec Model\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19759\n",
      "           1       1.00      1.00      1.00     19906\n",
      "\n",
      "    accuracy                           1.00     39665\n",
      "   macro avg       1.00      1.00      1.00     39665\n",
      "weighted avg       1.00      1.00      1.00     39665\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.73      0.73      4939\n",
      "           1       0.73      0.72      0.72      4978\n",
      "\n",
      "    accuracy                           0.73      9917\n",
      "   macro avg       0.73      0.73      0.73      9917\n",
      "weighted avg       0.73      0.73      0.73      9917\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "dt_bow = DecisionTreeClassifier(random_state=0).fit(bow_x_train, bow_y_train)\n",
    "\n",
    "print(\"Bag of Words Model\")\n",
    "print(\" \")\n",
    "print(classification_report(bow_y_train,dt_bow.predict(bow_x_train)))\n",
    "print(classification_report(bow_y_test,dt_bow.predict(bow_x_test)))\n",
    "\n",
    "\n",
    "dt_tf_idf = DecisionTreeClassifier(random_state=0).fit(tfidf_x_train, tfidf_y_train)\n",
    "\n",
    "print(\"Term Frequency Inverse Document Frequency Model\")\n",
    "print(\" \")\n",
    "print(classification_report(tfidf_y_train,dt_tf_idf.predict(tfidf_x_train)))\n",
    "print(classification_report(tfidf_y_test,dt_tf_idf.predict(tfidf_x_test)))\n",
    "\n",
    "\n",
    "dt_word2vec = DecisionTreeClassifier(random_state=0).fit(word2vec_x_train, word2vec_y_train)\n",
    "\n",
    "print(\"Word2vec Model\")\n",
    "print(\" \")\n",
    "print(classification_report(word2vec_y_train,dt_word2vec.predict(word2vec_x_train)))\n",
    "print(classification_report(word2vec_y_test,dt_word2vec.predict(word2vec_x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:51:57.702462Z",
     "iopub.status.busy": "2022-11-29T11:51:57.702016Z",
     "iopub.status.idle": "2022-11-29T11:57:02.600788Z",
     "shell.execute_reply": "2022-11-29T11:57:02.599418Z",
     "shell.execute_reply.started": "2022-11-29T11:51:57.702435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Model\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19759\n",
      "           1       1.00      1.00      1.00     19906\n",
      "\n",
      "    accuracy                           1.00     39665\n",
      "   macro avg       1.00      1.00      1.00     39665\n",
      "weighted avg       1.00      1.00      1.00     39665\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85      4939\n",
      "           1       0.85      0.85      0.85      4978\n",
      "\n",
      "    accuracy                           0.85      9917\n",
      "   macro avg       0.85      0.85      0.85      9917\n",
      "weighted avg       0.85      0.85      0.85      9917\n",
      "\n",
      "Term Frequency Inverse Document Frequency Model\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19759\n",
      "           1       1.00      1.00      1.00     19906\n",
      "\n",
      "    accuracy                           1.00     39665\n",
      "   macro avg       1.00      1.00      1.00     39665\n",
      "weighted avg       1.00      1.00      1.00     39665\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.85      4939\n",
      "           1       0.85      0.85      0.85      4978\n",
      "\n",
      "    accuracy                           0.85      9917\n",
      "   macro avg       0.85      0.85      0.85      9917\n",
      "weighted avg       0.85      0.85      0.85      9917\n",
      "\n",
      "Word2vec Model\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19759\n",
      "           1       1.00      1.00      1.00     19906\n",
      "\n",
      "    accuracy                           1.00     39665\n",
      "   macro avg       1.00      1.00      1.00     39665\n",
      "weighted avg       1.00      1.00      1.00     39665\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.81      0.83      4939\n",
      "           1       0.82      0.85      0.84      4978\n",
      "\n",
      "    accuracy                           0.83      9917\n",
      "   macro avg       0.83      0.83      0.83      9917\n",
      "weighted avg       0.83      0.83      0.83      9917\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf_bow = RandomForestClassifier(random_state=0).fit(bow_x_train, bow_y_train)\n",
    "\n",
    "print(\"Bag of Words Model\")\n",
    "print(\" \")\n",
    "print(classification_report(bow_y_train,rf_bow.predict(bow_x_train)))\n",
    "print(classification_report(bow_y_test,rf_bow.predict(bow_x_test)))\n",
    "\n",
    "\n",
    "rf_tf_idf = RandomForestClassifier(random_state=0).fit(tfidf_x_train, tfidf_y_train)\n",
    "\n",
    "print(\"Term Frequency Inverse Document Frequency Model\")\n",
    "print(\" \")\n",
    "print(classification_report(tfidf_y_train,rf_tf_idf.predict(tfidf_x_train)))\n",
    "print(classification_report(tfidf_y_test,rf_tf_idf.predict(tfidf_x_test)))\n",
    "\n",
    "\n",
    "rf_word2vec = RandomForestClassifier(random_state=0).fit(word2vec_x_train, word2vec_y_train)\n",
    "\n",
    "print(\"Word2vec Model\")\n",
    "print(\" \")\n",
    "print(classification_report(word2vec_y_train,rf_word2vec.predict(word2vec_x_train)))\n",
    "print(classification_report(word2vec_y_test,rf_word2vec.predict(word2vec_x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T11:57:02.603283Z",
     "iopub.status.busy": "2022-11-29T11:57:02.602780Z",
     "iopub.status.idle": "2022-11-29T12:02:13.606884Z",
     "shell.execute_reply": "2022-11-29T12:02:13.605984Z",
     "shell.execute_reply.started": "2022-11-29T11:57:02.603258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Model\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.81     20039\n",
      "           1       0.82      0.78      0.80     19961\n",
      "\n",
      "    accuracy                           0.80     40000\n",
      "   macro avg       0.80      0.80      0.80     40000\n",
      "weighted avg       0.80      0.80      0.80     40000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.65      0.63      4961\n",
      "           1       0.64      0.61      0.62      5039\n",
      "\n",
      "    accuracy                           0.63     10000\n",
      "   macro avg       0.63      0.63      0.63     10000\n",
      "weighted avg       0.63      0.63      0.63     10000\n",
      "\n",
      "Term Frequency Inverse Document Frequency Model\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.84      0.88     20039\n",
      "           1       0.85      0.93      0.89     19961\n",
      "\n",
      "    accuracy                           0.88     40000\n",
      "   macro avg       0.89      0.88      0.88     40000\n",
      "weighted avg       0.89      0.88      0.88     40000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.70      0.75      4961\n",
      "           1       0.74      0.83      0.78      5039\n",
      "\n",
      "    accuracy                           0.77     10000\n",
      "   macro avg       0.77      0.77      0.77     10000\n",
      "weighted avg       0.77      0.77      0.77     10000\n",
      "\n",
      "Word2vec Model\n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.89     20039\n",
      "           1       0.89      0.88      0.89     19961\n",
      "\n",
      "    accuracy                           0.89     40000\n",
      "   macro avg       0.89      0.89      0.89     40000\n",
      "weighted avg       0.89      0.89      0.89     40000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.80      0.79      4961\n",
      "           1       0.80      0.78      0.79      5039\n",
      "\n",
      "    accuracy                           0.79     10000\n",
      "   macro avg       0.79      0.79      0.79     10000\n",
      "weighted avg       0.79      0.79      0.79     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_bow = KNeighborsClassifier(n_neighbors=3).fit(bow_x_train, bow_y_train)\n",
    "\n",
    "print(\"Bag of Words Model\")\n",
    "print(\" \")\n",
    "print(classification_report(bow_y_train,knn_bow.predict(bow_x_train)))\n",
    "print(classification_report(bow_y_test,knn_bow.predict(bow_x_test)))\n",
    "\n",
    "\n",
    "knn_tf_idf = KNeighborsClassifier(n_neighbors=3).fit(tfidf_x_train, tfidf_y_train)\n",
    "\n",
    "print(\"Term Frequency Inverse Document Frequency Model\")\n",
    "print(\" \")\n",
    "print(classification_report(tfidf_y_train,knn_tf_idf.predict(tfidf_x_train)))\n",
    "print(classification_report(tfidf_y_test,knn_tf_idf.predict(tfidf_x_test)))\n",
    "\n",
    "\n",
    "knn_word2vec = KNeighborsClassifier(n_neighbors=3).fit(word2vec_x_train, word2vec_y_train)\n",
    "\n",
    "print(\"Word2vec Model\")\n",
    "print(\" \")\n",
    "print(classification_report(word2vec_y_train,knn_word2vec.predict(word2vec_x_train)))\n",
    "print(classification_report(word2vec_y_test,knn_word2vec.predict(word2vec_x_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
